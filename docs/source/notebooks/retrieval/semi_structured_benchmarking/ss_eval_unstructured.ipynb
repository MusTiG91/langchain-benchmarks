{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c10b060d-6363-4669-8866-7c8ef1103a38",
   "metadata": {},
   "source": [
    "# Semi-structured eval: Unstructured + Multi-vector retriever\n",
    "\n",
    "We will test retrival of table information from the `Semi-structured Reports` dataset using various methods.\n",
    "\n",
    "## Pre-requisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae46dade-a12b-4ee4-abfa-c5b071a7eb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U langchain langsmith langchain_benchmarks\n",
    "%pip install --quiet chromadb openai \"unstructured[all-docs]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daca152-b63b-4403-9b46-66ff9ab2a275",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dafb546c-32a1-46e9-b0aa-9b4b2c439324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain_benchmarks import clone_public_dataset, registry\n",
    "from langchain_benchmarks.rag.tasks.semi_structured_reports import get_file_names\n",
    "\n",
    "# Task\n",
    "task = registry[\"Semi-structured Reports\"]\n",
    "\n",
    "# Files used\n",
    "paths = list(get_file_names())\n",
    "files = [str(p) for p in paths]\n",
    "\n",
    "### TODO: Replace when dataset is updated \n",
    "dir = \"/Users/rlm/Desktop/Eval_Sets/semi_structured_reports/\"\n",
    "files = [f for f in os.listdir(dir) if f.endswith(\".pdf\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddce580b-cdbf-4e09-8273-35a9e0b62307",
   "metadata": {},
   "source": [
    "### Load\n",
    "\n",
    "Use table-aware splitting following cookbook [here](https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_Structured_RAG.ipynb).\n",
    "\n",
    "In addition to the below pip packages, you will also need [poppler](https://pdf2image.readthedocs.io/en/latest/installation.html) and [tesseract](https://tesseract-ocr.github.io/tessdoc/Installation.html) in your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63985459-b295-422a-a5ca-1fba86049964",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "\n",
    "def categorize_elements(raw_pdf_elements):\n",
    "    \"\"\"\n",
    "    Categorize extracted elements from a PDF into tables and texts.\n",
    "    raw_pdf_elements: List of unstructured.documents.elements\n",
    "    \"\"\"\n",
    "    tables = []\n",
    "    texts = []\n",
    "    for element in raw_pdf_elements:\n",
    "        if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "            tables.append(str(element))\n",
    "        elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
    "            texts.append(str(element))\n",
    "    return texts, tables\n",
    "\n",
    "\n",
    "def parse_pdf_unstructured(file):\n",
    "    # Get elements\n",
    "    unstructured_elements = partition_pdf(\n",
    "        filename=file,\n",
    "        extract_images_in_pdf=False,\n",
    "        infer_table_structure=True,\n",
    "        chunking_strategy=\"by_title\",\n",
    "        max_characters=4000,\n",
    "        new_after_n_chars=3800,\n",
    "        combine_text_under_n_chars=2000,\n",
    "        image_output_dir_path=os.path.dirname(file),\n",
    "    )\n",
    "\n",
    "    # Categorize elements by type\n",
    "    texts, tables = categorize_elements(unstructured_elements)\n",
    "    print(f\"There are {len(tables)} tables\")\n",
    "    print(f\"There are {len(texts)} text elements\")\n",
    "    return texts, tables\n",
    "\n",
    "\n",
    "texts = []\n",
    "tables = []\n",
    "for fi in files:\n",
    "    texts, tables = parse_pdf_unstructured(dir+fi)\n",
    "    texts.extend(texts)\n",
    "    tables.extend(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad81cabc-17b9-4dcf-bfe2-6deffc4d177a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Optional: Enforce a specific token size for texts\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=4000, chunk_overlap=0\n",
    ")\n",
    "joined_texts = \" \".join(texts)\n",
    "texts = text_splitter.split_text(joined_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d8e2db-98da-4867-9c1a-c1d59f2b3d41",
   "metadata": {},
   "source": [
    "## Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c898d14-1ca6-477f-93bd-3ec942f02748",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.schema.document import Document\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.storage import InMemoryStore\n",
    "\n",
    "\n",
    "def generate_text_summaries(texts, tables, summarize_texts=False):\n",
    "    \"\"\"\n",
    "    Summarize text elements\n",
    "    texts: List of str\n",
    "    tables: List of str\n",
    "    summarize_texts: Bool to summarize texts\n",
    "    \"\"\"\n",
    "\n",
    "    # Prompt\n",
    "    prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw text or table elements. \\\n",
    "    Give a concise summary of the table or text that is well optimized for retrieval. Table or text: {element} \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "    # Text summary chain\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
    "    summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n",
    "\n",
    "    # Initialize empty summaries\n",
    "    text_summaries = []\n",
    "    table_summaries = []\n",
    "\n",
    "    # Apply to text if texts are provided and summarization is requested\n",
    "    if texts and summarize_texts:\n",
    "        text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 5})\n",
    "    elif texts:\n",
    "        text_summaries = texts\n",
    "\n",
    "    # Apply to tables if tables are provided\n",
    "    if tables:\n",
    "        table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 5})\n",
    "\n",
    "    return text_summaries, table_summaries\n",
    "\n",
    "\n",
    "def create_multi_vector_retriever(\n",
    "    vectorstore,\n",
    "    text_summaries=None,\n",
    "    texts=None,\n",
    "    table_summaries=None,\n",
    "    tables=None,\n",
    "    image_summaries=None,\n",
    "    images=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create retriever that indexes summaries, but returns raw images or texts\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the storage layer\n",
    "    store = InMemoryStore()\n",
    "    id_key = \"doc_id\"\n",
    "\n",
    "    # Create the multi-vector retriever\n",
    "    retriever = MultiVectorRetriever(\n",
    "        vectorstore=vectorstore,\n",
    "        docstore=store,\n",
    "        id_key=id_key,\n",
    "    )\n",
    "\n",
    "    # Helper function to add documents to the vectorstore and docstore\n",
    "    def add_documents(retriever, doc_summaries, doc_contents):\n",
    "        doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n",
    "        summary_docs = [\n",
    "            Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "            for i, s in enumerate(doc_summaries)\n",
    "        ]\n",
    "        retriever.vectorstore.add_documents(summary_docs)\n",
    "        retriever.docstore.mset(list(zip(doc_ids, doc_contents)))\n",
    "\n",
    "    # Add texts, tables, and images\n",
    "    if text_summaries:\n",
    "        add_documents(retriever, text_summaries, texts)\n",
    "    if table_summaries:\n",
    "        add_documents(retriever, table_summaries, tables)\n",
    "    if image_summaries:\n",
    "        add_documents(retriever, image_summaries, images)\n",
    "\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e3bee7d-8cf4-4896-8111-323f4ddcbfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get text, table summaries\n",
    "text_summaries, table_summaries = generate_text_summaries(\n",
    "    texts, tables, summarize_texts=False\n",
    ")\n",
    "\n",
    "# The vectorstore to use to index the summaries\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"unstructured\", embedding_function=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "# Create retriever\n",
    "retriever_unstructured = create_multi_vector_retriever(\n",
    "    vectorstore,\n",
    "    text_summaries,\n",
    "    texts,\n",
    "    table_summaries,\n",
    "    tables,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb11d59-677a-47a8-8225-0be5c85325db",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "705a9e67-09b1-47a6-8280-db4a77052a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "def rag_chain(retriever):\n",
    "    \"\"\"\n",
    "    RAG chain\n",
    "    \"\"\"\n",
    "\n",
    "    # Prompt template\n",
    "    template = \"\"\"Answer the question based only on the following context, which can include text and tables:\n",
    "    {context}\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    # LLM\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
    "\n",
    "    # RAG pipeline\n",
    "    chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    return chain\n",
    "\n",
    "\n",
    "# Create RAG chain\n",
    "chain = rag_chain(retriever_unstructured)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc35eefd-eef1-4c51-a5d8-03f45b2b21d9",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eec0906a-1eaf-415e-9fcd-06e2efb9ecdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'unstructured_4af974d8-5489-42fd-af52-1a6b461f3a32' at:\n",
      "https://smith.langchain.com/o/1fa8b1f4-fcb9-4072-9aa9-983e35ad61b8/projects/p/90d8f6c5-448a-4c2e-b0a0-bc84ca6b2b10?eval=true\n",
      "\n",
      "View all tests for Dataset Semi-Structured-Eval at:\n",
      "https://smith.langchain.com/o/1fa8b1f4-fcb9-4072-9aa9-983e35ad61b8/datasets/95f61109-029d-43a9-ae7d-ec1d53c6f723\n",
      "[------------------------------------------------->] 20/20\n",
      " Eval quantiles:\n",
      "                               0.25       0.5      0.75      mean      mode\n",
      "embedding_cosine_distance  0.103910  0.114067  0.121735  0.103511  0.035518\n",
      "score_string:accuracy      0.100000  0.100000  0.100000  0.240000  0.100000\n",
      "faithfulness               0.925000  1.000000  1.000000  0.835000  1.000000\n",
      "execution_time             7.441979  7.441979  7.441979  7.441979  7.441979\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "from langsmith.client import Client\n",
    "\n",
    "from langchain_benchmarks.rag import get_eval_config\n",
    "\n",
    "\n",
    "def run_eval(chain, eval_run_name):\n",
    "    \"\"\"\n",
    "    Run eval\n",
    "    \"\"\"\n",
    "    client = Client()\n",
    "    test_run = client.run_on_dataset(\n",
    "        ### TODO: Replace with public dataset \n",
    "        dataset_name=\"Semi-Structured-Eval\",\n",
    "        llm_or_chain_factory=lambda: (lambda x: x[\"question\"]) | chain,\n",
    "        evaluation=get_eval_config(),\n",
    "        verbose=True,\n",
    "        project_name = eval_run_name\n",
    "    )\n",
    "\n",
    "\n",
    "# Experiments\n",
    "chain_map = {\n",
    "    \"unstructured\": chain,\n",
    "}\n",
    "\n",
    "run_id = str(uuid.uuid4())\n",
    "for project_name, chain in chain_map.items():\n",
    "    run_eval(chain, project_name+\"_\"+run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81555bb2-be2e-4a57-b3f7-5a96419f9140",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
