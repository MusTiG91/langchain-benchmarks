{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "684707be-0b26-417f-92d5-5e11fc49a923",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "### Get Public Dataset\n",
    "\n",
    "Get the LangSmith public dataset for semi-structured data, `Semi-structured Reports`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c213899-73dd-4043-b331-c15d818e59a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain_benchmarks import clone_public_dataset, registry\n",
    "from langchain_benchmarks.rag.tasks.semi_structured_reports import get_file_names\n",
    "\n",
    "# Task\n",
    "task = registry[\"Semi-structured Reports\"]\n",
    "\n",
    "# Files used\n",
    "paths = list(get_file_names())\n",
    "files = [str(p) for p in paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2fc5fc-e8c9-4086-8aa8-37a2eb401cc1",
   "metadata": {},
   "source": [
    "### Base Case\n",
    "\n",
    "PDF loader (naive to tables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23948b39-e22d-4233-bc4c-1173a0bded0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11 text elements\n",
      "There are 3 text elements\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "\n",
    "def load_and_split(file):\n",
    "    \"\"\"\n",
    "    Load and split PDF files\n",
    "    \"\"\"\n",
    "\n",
    "    loader = PyPDFLoader(file)\n",
    "    pdf_pages = loader.load()\n",
    "\n",
    "    # Split\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=2000, chunk_overlap=50\n",
    "    )\n",
    "\n",
    "    # Get chunks\n",
    "    docs = text_splitter.split_documents(pdf_pages)\n",
    "    texts = [d.page_content for d in docs]\n",
    "    print(f\"There are {len(texts)} text elements\")\n",
    "    return texts\n",
    "\n",
    "\n",
    "baseline_texts = []\n",
    "for fi in files:\n",
    "    baseline_texts.append(load_and_split(fi))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dcb645-763a-44b6-883e-de55fe8df811",
   "metadata": {},
   "source": [
    "### Unstructured\n",
    "\n",
    "Table-aware splitting following cookbook [here](https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_Structured_RAG.ipynb).\n",
    "\n",
    "In addition to the below pip packages, you will also need [poppler](https://pdf2image.readthedocs.io/en/latest/installation.html) and [tesseract](https://tesseract-ocr.github.io/tessdoc/Installation.html) in your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9c71ed-985d-44ce-9770-e100b78ba3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install \"unstructured[all-docs]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90d2fbe3-c834-4119-9d53-bb3881b3d2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/table-transformer-structure-recognition were not used when initializing TableTransformerForObjectDetection: ['model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6 tables\n",
      "There are 12 text elements\n",
      "There are 1 tables\n",
      "There are 5 text elements\n"
     ]
    }
   ],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "\n",
    "def categorize_elements(raw_pdf_elements):\n",
    "    \"\"\"\n",
    "    Categorize extracted elements from a PDF into tables and texts.\n",
    "    raw_pdf_elements: List of unstructured.documents.elements\n",
    "    \"\"\"\n",
    "    tables = []\n",
    "    texts = []\n",
    "    for element in raw_pdf_elements:\n",
    "        if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "            tables.append(str(element))\n",
    "        elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
    "            texts.append(str(element))\n",
    "    return texts, tables\n",
    "\n",
    "\n",
    "def parse_pdf_unstructured(file):\n",
    "    # Get elements\n",
    "    unstructured_elements = partition_pdf(\n",
    "        filename=file,\n",
    "        extract_images_in_pdf=False,\n",
    "        infer_table_structure=True,\n",
    "        chunking_strategy=\"by_title\",\n",
    "        max_characters=4000,\n",
    "        new_after_n_chars=3800,\n",
    "        combine_text_under_n_chars=2000,\n",
    "        image_output_dir_path=os.path.dirname(file),\n",
    "    )\n",
    "\n",
    "    # Categorize elements by type\n",
    "    texts, tables = categorize_elements(unstructured_elements)\n",
    "    print(f\"There are {len(tables)} tables\")\n",
    "    print(f\"There are {len(texts)} text elements\")\n",
    "    return texts, tables\n",
    "\n",
    "\n",
    "# Run\n",
    "unstructured_texts = []\n",
    "unstructured_tables = []\n",
    "for fi in files:\n",
    "    texts, tables = parse_pdf_unstructured(fi)\n",
    "    unstructured_texts.append(texts)\n",
    "    unstructured_tables.append(tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a1432f-654d-4b0c-9f92-14c7cada9237",
   "metadata": {},
   "source": [
    "### Docugami\n",
    "\n",
    "Table-aware splitting following cookbook [here](https://github.com/langchain-ai/langchain/blob/master/cookbook/docugami_xml_kg_rag.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b57330a-0b43-4135-9901-05002ce09af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install docugami==0.0.4 dgml-utils==0.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e09cbb-5758-4ff9-bce5-80730ab2313c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docugami_processing import upload_files, wait_for_xml\n",
    "\n",
    "# Load\n",
    "DOCSET_NAME = \"Semi-Structured\"\n",
    "dg_docs = upload_files(files, DOCSET_NAME)\n",
    "dgml_paths = wait_for_xml(dg_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799ca7a2-3907-456a-be4c-7de6a43c930e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_docugami_file(dgml_path):\n",
    "    with open(dgml_path, \"r\") as file:\n",
    "        contents = file.read().encode(\"utf-8\")\n",
    "        chunks = get_chunks_str(\n",
    "            contents,\n",
    "            include_xml_tags=True,\n",
    "            min_text_length=1024 * 8,  # 8k chars are ~2k tokens\n",
    "            max_text_length=1024 * 8,  # 8k chars are ~2k tokens\n",
    "        )\n",
    "\n",
    "    # Tables\n",
    "    tables = [c for c in chunks if \"table\" in c.structure.split()]\n",
    "    print(f\"There are {len(tables)} tables\")\n",
    "\n",
    "    # Text\n",
    "    texts = [c for c in chunks if \"table\" not in c.structure.split()]\n",
    "    print(f\"There are {len(texts)} text elements\")\n",
    "    return texts, tables\n",
    "\n",
    "\n",
    "docugami_texts = []\n",
    "docugami_tables = []\n",
    "for fname in files:\n",
    "    # Get xml\n",
    "    dgml_path = dgml_paths[fname]\n",
    "    # Extract elelemtns\n",
    "    texts, tables = extract_docugami_file(dgml_path)\n",
    "    docugami_texts.append(texts)\n",
    "    docugami_tables.append(tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587675d9-c68e-4691-8a6c-796a1c113e6e",
   "metadata": {},
   "source": [
    "## Indexing\n",
    "\n",
    "### Base case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fc789b-1be0-4944-8e8d-a55d2cbe7b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore_baseline = Chroma.from_texts(\n",
    "    documents=baseline_texts, collection_name=\"baseline\", embedding=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "retriever_baseline = vectorstore_baseline.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ab563f-b52e-4bf8-8efb-d16f0fe5179b",
   "metadata": {},
   "source": [
    "### Multi-vector retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1580144d-4931-4163-be64-8612d2e4e763",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_summaries(texts, tables, summarize_texts=False):\n",
    "    \"\"\"\n",
    "    Summarize text elements\n",
    "    texts: List of str\n",
    "    tables: List of str\n",
    "    summarize_texts: Bool to summarize texts\n",
    "    \"\"\"\n",
    "\n",
    "    # Prompt\n",
    "    prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw text or table elements. \\\n",
    "    Give a concise summary of the table or text that is well optimized for retrieval. Table or text: {element} \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "    # Text summary chain\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
    "    summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n",
    "\n",
    "    # Initialize empty summaries\n",
    "    text_summaries = []\n",
    "    table_summaries = []\n",
    "\n",
    "    # Apply to text if texts are provided and summarization is requested\n",
    "    if texts and summarize_texts:\n",
    "        text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 5})\n",
    "    elif texts:\n",
    "        text_summaries = texts\n",
    "\n",
    "    # Apply to tables if tables are provided\n",
    "    if tables:\n",
    "        table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 5})\n",
    "\n",
    "    return text_summaries, table_summaries\n",
    "\n",
    "\n",
    "def create_multi_vector_retriever(\n",
    "    vectorstore, text_summaries, texts, table_summaries, tables, image_summaries, images\n",
    "):\n",
    "    \"\"\"\n",
    "    Create retriever that indexes summaries, but returns raw images or texts\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the storage layer\n",
    "    store = InMemoryStore()\n",
    "    id_key = \"doc_id\"\n",
    "\n",
    "    # Create the multi-vector retriever\n",
    "    retriever = MultiVectorRetriever(\n",
    "        vectorstore=vectorstore,\n",
    "        docstore=store,\n",
    "        id_key=id_key,\n",
    "    )\n",
    "\n",
    "    # Helper function to add documents to the vectorstore and docstore\n",
    "    def add_documents(retriever, doc_summaries, doc_contents):\n",
    "        doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n",
    "        summary_docs = [\n",
    "            Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "            for i, s in enumerate(doc_summaries)\n",
    "        ]\n",
    "        retriever.vectorstore.add_documents(summary_docs)\n",
    "        retriever.docstore.mset(list(zip(doc_ids, doc_contents)))\n",
    "\n",
    "    # Add texts, tables, and images\n",
    "    if text_summaries:\n",
    "        add_documents(retriever, text_summaries, texts)\n",
    "    if table_summaries:\n",
    "        add_documents(retriever, table_summaries, tables)\n",
    "    if image_summaries:\n",
    "        add_documents(retriever, image_summaries, images)\n",
    "\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10967672-aa36-4405-9d85-701e28d79940",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Unstructured\n",
    "\n",
    "# Get text, table summaries\n",
    "unstructured_text_summaries, unstructured_table_summaries = generate_text_summaries(\n",
    "    unstructured_texts, unstructured_tables, summarize_texts=False\n",
    ")\n",
    "\n",
    "# The vectorstore to use to index the summaries\n",
    "unstructured_vectorstore = Chroma(\n",
    "    collection_name=\"unstructured\", embedding_function=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "# Create retriever\n",
    "retriever_unstructured = create_multi_vector_retriever(\n",
    "    unstructured_vectorstore,\n",
    "    unstructured_text_summaries,\n",
    "    unstructured_texts,\n",
    "    unstructured_table_summaries,\n",
    "    unstructured_tables,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea447967-e16f-4252-999d-08b44e4a2643",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Docugami\n",
    "\n",
    "# Get text, table summaries\n",
    "docugami_text_summaries, docugami_table_summaries = generate_text_summaries(\n",
    "    docugami_texts, docugami_tables, summarize_texts=False\n",
    ")\n",
    "\n",
    "# The vectorstore to use to index the summaries\n",
    "docugami_vectorstore = Chroma(\n",
    "    collection_name=\"docugami\", embedding_function=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "# Create retriever\n",
    "retriever_docugami = create_multi_vector_retriever(\n",
    "    docugami_vectorstore,\n",
    "    docugami_text_summaries,\n",
    "    docugami_texts,\n",
    "    docugami_table_summaries,\n",
    "    docugami_tables,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a94ea8f-eb08-492b-90db-96e27ccd5b61",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc530b9-c804-4354-967f-4342431eccd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_chain(retriever):\n",
    "    \"\"\"\n",
    "    RAG chain\n",
    "    \"\"\"\n",
    "\n",
    "    # Prompt template\n",
    "    template = \"\"\"Answer the question based only on the following context, which can include text and tables:\n",
    "    {context}\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    # LLM\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
    "\n",
    "    # RAG pipeline\n",
    "    baseline_chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    return chain\n",
    "\n",
    "\n",
    "# Create RAG chains\n",
    "chain_baseline = rag_chain(retriever_baseline)\n",
    "chain_unstructured = rag_chain(retriever_unstructured)\n",
    "chain_docugami = rag_chain(retriever_docugami)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ebcbab-3207-4012-82b2-9d71d0f0edb1",
   "metadata": {},
   "source": [
    "# Eval\n",
    "\n",
    "See guide [here](https://github.com/langchain-ai/langchain-benchmarks/blob/main/docs/source/notebooks/retrieval/semi_structured.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d729313-5f39-4a4d-87d3-8cd837ad7d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from langsmith.client import Client\n",
    "\n",
    "from langchain_benchmarks.rag import get_eval_config\n",
    "\n",
    "\n",
    "def run_eval(chain, eval_run_name):\n",
    "    \"\"\"\n",
    "    Run eval\n",
    "    \"\"\"\n",
    "    client = Client()\n",
    "    test_run = client.run_on_dataset(\n",
    "        dataset_name=task.name,\n",
    "        llm_or_chain_factory=chain,\n",
    "        evaluation=get_eval_config(),\n",
    "        verbose=True,\n",
    "        project_name=eval_run_name,\n",
    "    )\n",
    "\n",
    "\n",
    "# Experiments\n",
    "chain_map = {\n",
    "    \"baseline\": chain_baseline,\n",
    "    \"unstructured\": chain_unstructured,\n",
    "    \"docugami\": chain_docugami,\n",
    "    ### TODO: Add images\n",
    "}\n",
    "\n",
    "for project_name, chain in chain_map.items():\n",
    "    run_eval(chain, project_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
