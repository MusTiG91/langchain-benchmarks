{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdf338c7-557b-420e-ae99-3868f3febfaa",
   "metadata": {},
   "source": [
    "# Semi-structured retrieval\n",
    "\n",
    "We will test retrival of table information from the `Semi-structured Reports`dataset using various methods.\n",
    "\n",
    "## Pre-requisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da64ebf7-1b59-404f-9a76-b79893f2240d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U langchain langsmith langchainhub  langchain_benchmarks\n",
    "%pip install --quiet chromadb openai \"unstructured[all-docs]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684707be-0b26-417f-92d5-5e11fc49a923",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "### Get Public Dataset\n",
    "\n",
    "Get the LangSmith public dataset for semi-structured data, `Semi-structured Reports`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c213899-73dd-4043-b331-c15d818e59a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /Users/rlm/miniforge3/envs/llama2/lib/python3.9/site-packages/langchain_benchmarks/rag/tasks/semi_structured_reports/indexing/chroma_db.zip does not exist. Downloading from GCS...\n",
      "File https://storage.googleapis.com/benchmarks-artifacts/langchain-docs-benchmarking/semi_structured_earnings.zip downloaded.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain_benchmarks import clone_public_dataset, registry\n",
    "from langchain_benchmarks.rag.tasks.semi_structured_reports import get_file_names\n",
    "\n",
    "# Task\n",
    "task = registry[\"Semi-structured Reports\"]\n",
    "\n",
    "# Files used\n",
    "paths = list(get_file_names())\n",
    "files = [str(p) for p in paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2fc5fc-e8c9-4086-8aa8-37a2eb401cc1",
   "metadata": {},
   "source": [
    "### Base Case\n",
    "\n",
    "Use PDF loader, which is naive to tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23948b39-e22d-4233-bc4c-1173a0bded0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11 text elements\n",
      "There are 3 text elements\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "def load_and_split(file):\n",
    "    \"\"\"\n",
    "    Load and split PDF files\n",
    "    \"\"\"\n",
    "\n",
    "    loader = PyPDFLoader(file)\n",
    "    pdf_pages = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=2000, chunk_overlap=50\n",
    "    )\n",
    "\n",
    "    # Get chunks\n",
    "    docs = text_splitter.split_documents(pdf_pages)\n",
    "    texts = [d.page_content for d in docs]\n",
    "    print(f\"There are {len(texts)} text elements\")\n",
    "    return texts\n",
    "\n",
    "\n",
    "baseline_texts = []\n",
    "for fi in files:\n",
    "    baseline_texts.extend(load_and_split(fi))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dcb645-763a-44b6-883e-de55fe8df811",
   "metadata": {},
   "source": [
    "### Unstructured\n",
    "\n",
    "Use tabble-aware splitting following cookbook [here](https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_Structured_RAG.ipynb).\n",
    "\n",
    "In addition to the below pip packages, you will also need [poppler](https://pdf2image.readthedocs.io/en/latest/installation.html) and [tesseract](https://tesseract-ocr.github.io/tessdoc/Installation.html) in your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90d2fbe3-c834-4119-9d53-bb3881b3d2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6 tables\n",
      "There are 12 text elements\n",
      "There are 1 tables\n",
      "There are 5 text elements\n"
     ]
    }
   ],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "\n",
    "def categorize_elements(raw_pdf_elements):\n",
    "    \"\"\"\n",
    "    Categorize extracted elements from a PDF into tables and texts.\n",
    "    raw_pdf_elements: List of unstructured.documents.elements\n",
    "    \"\"\"\n",
    "    tables = []\n",
    "    texts = []\n",
    "    for element in raw_pdf_elements:\n",
    "        if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "            tables.append(str(element))\n",
    "        elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
    "            texts.append(str(element))\n",
    "    return texts, tables\n",
    "\n",
    "\n",
    "def parse_pdf_unstructured(file):\n",
    "    # Get elements\n",
    "    unstructured_elements = partition_pdf(\n",
    "        filename=file,\n",
    "        extract_images_in_pdf=False,\n",
    "        infer_table_structure=True,\n",
    "        chunking_strategy=\"by_title\",\n",
    "        max_characters=4000,\n",
    "        new_after_n_chars=3800,\n",
    "        combine_text_under_n_chars=2000,\n",
    "        image_output_dir_path=os.path.dirname(file),\n",
    "    )\n",
    "\n",
    "    # Categorize elements by type\n",
    "    texts, tables = categorize_elements(unstructured_elements)\n",
    "    print(f\"There are {len(tables)} tables\")\n",
    "    print(f\"There are {len(texts)} text elements\")\n",
    "    return texts, tables\n",
    "\n",
    "\n",
    "unstructured_texts = []\n",
    "unstructured_tables = []\n",
    "for fi in files:\n",
    "    texts, tables = parse_pdf_unstructured(fi)\n",
    "    unstructured_texts.extend(texts)\n",
    "    unstructured_tables.extend(tables)\n",
    "\n",
    "# Optional: Enforce a specific token size for texts\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=4000, chunk_overlap=0\n",
    ")\n",
    "joined_texts = \" \".join(unstructured_texts)\n",
    "unstructured_texts_4k_token = text_splitter.split_text(joined_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a1432f-654d-4b0c-9f92-14c7cada9237",
   "metadata": {},
   "source": [
    "### Docugami\n",
    "\n",
    "**-- TO-DO --**\n",
    "\n",
    "Use table-aware splitting following cookbook [here](https://github.com/langchain-ai/langchain/blob/master/cookbook/docugami_xml_kg_rag.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b57330a-0b43-4135-9901-05002ce09af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install docugami==0.0.4 dgml-utils==0.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e09cbb-5758-4ff9-bce5-80730ab2313c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docugami_processing import upload_files, wait_for_xml\n",
    "\n",
    "# Load\n",
    "DOCSET_NAME = \"Semi-Structured\"\n",
    "dg_docs = upload_files(files, DOCSET_NAME)\n",
    "dgml_paths = wait_for_xml(dg_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799ca7a2-3907-456a-be4c-7de6a43c930e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_docugami_file(dgml_path):\n",
    "    with open(dgml_path, \"r\") as file:\n",
    "        contents = file.read().encode(\"utf-8\")\n",
    "        chunks = get_chunks_str(\n",
    "            contents,\n",
    "            include_xml_tags=True,\n",
    "            min_text_length=1024 * 8,  # 8k chars are ~2k tokens\n",
    "            max_text_length=1024 * 8,  # 8k chars are ~2k tokens\n",
    "        )\n",
    "\n",
    "    # Tables\n",
    "    tables = [c for c in chunks if \"table\" in c.structure.split()]\n",
    "    print(f\"There are {len(tables)} tables\")\n",
    "\n",
    "    # Text\n",
    "    texts = [c for c in chunks if \"table\" not in c.structure.split()]\n",
    "    print(f\"There are {len(texts)} text elements\")\n",
    "    return texts, tables\n",
    "\n",
    "\n",
    "docugami_texts = []\n",
    "docugami_tables = []\n",
    "for fname in files:\n",
    "    # Get xml\n",
    "    dgml_path = dgml_paths[fname]\n",
    "    # Extract elelemtns\n",
    "    texts, tables = extract_docugami_file(dgml_path)\n",
    "    docugami_texts.extend(texts)\n",
    "    docugami_tables.extend(tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587675d9-c68e-4691-8a6c-796a1c113e6e",
   "metadata": {},
   "source": [
    "## Indexing\n",
    "\n",
    "### Base case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80fc789b-1be0-4944-8e8d-a55d2cbe7b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "vectorstore_baseline = Chroma.from_texts(\n",
    "    texts=baseline_texts, collection_name=\"baseline\", embedding=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "retriever_baseline = vectorstore_baseline.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ab563f-b52e-4bf8-8efb-d16f0fe5179b",
   "metadata": {},
   "source": [
    "### Multi-vector retriever w/ text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1580144d-4931-4163-be64-8612d2e4e763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.schema.document import Document\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.storage import InMemoryStore\n",
    "\n",
    "\n",
    "def generate_text_summaries(texts, tables, summarize_texts=False):\n",
    "    \"\"\"\n",
    "    Summarize text elements\n",
    "    texts: List of str\n",
    "    tables: List of str\n",
    "    summarize_texts: Bool to summarize texts\n",
    "    \"\"\"\n",
    "\n",
    "    # Prompt\n",
    "    prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw text or table elements. \\\n",
    "    Give a concise summary of the table or text that is well optimized for retrieval. Table or text: {element} \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "    # Text summary chain\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
    "    summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n",
    "\n",
    "    # Initialize empty summaries\n",
    "    text_summaries = []\n",
    "    table_summaries = []\n",
    "\n",
    "    # Apply to text if texts are provided and summarization is requested\n",
    "    if texts and summarize_texts:\n",
    "        text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 5})\n",
    "    elif texts:\n",
    "        text_summaries = texts\n",
    "\n",
    "    # Apply to tables if tables are provided\n",
    "    if tables:\n",
    "        table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 5})\n",
    "\n",
    "    return text_summaries, table_summaries\n",
    "\n",
    "\n",
    "def create_multi_vector_retriever(\n",
    "    vectorstore,\n",
    "    text_summaries=None,\n",
    "    texts=None,\n",
    "    table_summaries=None,\n",
    "    tables=None,\n",
    "    image_summaries=None,\n",
    "    images=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create retriever that indexes summaries, but returns raw images or texts\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the storage layer\n",
    "    store = InMemoryStore()\n",
    "    id_key = \"doc_id\"\n",
    "\n",
    "    # Create the multi-vector retriever\n",
    "    retriever = MultiVectorRetriever(\n",
    "        vectorstore=vectorstore,\n",
    "        docstore=store,\n",
    "        id_key=id_key,\n",
    "    )\n",
    "\n",
    "    # Helper function to add documents to the vectorstore and docstore\n",
    "    def add_documents(retriever, doc_summaries, doc_contents):\n",
    "        doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n",
    "        summary_docs = [\n",
    "            Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "            for i, s in enumerate(doc_summaries)\n",
    "        ]\n",
    "        retriever.vectorstore.add_documents(summary_docs)\n",
    "        retriever.docstore.mset(list(zip(doc_ids, doc_contents)))\n",
    "\n",
    "    # Add texts, tables, and images\n",
    "    if text_summaries:\n",
    "        add_documents(retriever, text_summaries, texts)\n",
    "    if table_summaries:\n",
    "        add_documents(retriever, table_summaries, tables)\n",
    "    if image_summaries:\n",
    "        add_documents(retriever, image_summaries, images)\n",
    "\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10967672-aa36-4405-9d85-701e28d79940",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Unstructured table extraction + MV Retriever\n",
    "\n",
    "# Get text, table summaries\n",
    "unstructured_text_summaries, unstructured_table_summaries = generate_text_summaries(\n",
    "    unstructured_texts_4k_token, unstructured_tables, summarize_texts=False\n",
    ")\n",
    "\n",
    "# The vectorstore to use to index the summaries\n",
    "unstructured_vectorstore = Chroma(\n",
    "    collection_name=\"unstructured\", embedding_function=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "# Create retriever\n",
    "retriever_unstructured = create_multi_vector_retriever(\n",
    "    unstructured_vectorstore,\n",
    "    unstructured_text_summaries,\n",
    "    unstructured_texts_4k_token,\n",
    "    unstructured_table_summaries,\n",
    "    unstructured_tables,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c4ce47-f059-40e0-ad2e-530e4f07d5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Docugami table extraction + MV Retriever\n",
    "\n",
    "# Get text, table summaries\n",
    "docugami_text_summaries, docugami_table_summaries = generate_text_summaries(\n",
    "    docugami_texts, docugami_tables, summarize_texts=False\n",
    ")\n",
    "\n",
    "# The vectorstore to use to index the summaries\n",
    "docugami_vectorstore = Chroma(\n",
    "    collection_name=\"docugami\", embedding_function=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "# Create retriever\n",
    "retriever_docugami = create_multi_vector_retriever(\n",
    "    docugami_vectorstore,\n",
    "    docugami_text_summaries,\n",
    "    docugami_texts,\n",
    "    docugami_table_summaries,\n",
    "    docugami_tables,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880e1c11-16ad-44e8-9d1b-6e20bf14d524",
   "metadata": {},
   "source": [
    "### Multi-vector retriever w/ images\n",
    "\n",
    "Images extracted from the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "efb420ba-e2e8-4540-b9f5-57ad605ae5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "\n",
    "from langchain.schema.messages import HumanMessage\n",
    "\n",
    "\n",
    "def encode_image(image_path):\n",
    "    \"\"\"Getting the base64 string\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def image_summarize(img_base64, prompt):\n",
    "    \"\"\"Make image summary\"\"\"\n",
    "    chat = ChatOpenAI(model=\"gpt-4-vision-preview\", max_tokens=1024)\n",
    "\n",
    "    msg = chat.invoke(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                content=[\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"},\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return msg.content\n",
    "\n",
    "\n",
    "def generate_img_summaries(path):\n",
    "    \"\"\"\n",
    "    Generate summaries and base64 encoded strings for images\n",
    "    path: Path to list of .jpg files extracted by Unstructured\n",
    "    \"\"\"\n",
    "\n",
    "    # Store base64 encoded images\n",
    "    img_base64_list = []\n",
    "\n",
    "    # Store image summaries\n",
    "    image_summaries = []\n",
    "\n",
    "    # Prompt\n",
    "    prompt = \"\"\"You are an assistant tasked with summarizing images for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw image. \\\n",
    "    Give a concise summary of the image that is well optimized for retrieval.\"\"\"\n",
    "\n",
    "    # Apply to images\n",
    "    for img_file in sorted(os.listdir(path)):\n",
    "        if img_file.endswith(\".jpg\"):\n",
    "            img_path = os.path.join(path, img_file)\n",
    "            base64_image = encode_image(img_path)\n",
    "            img_base64_list.append(base64_image)\n",
    "            image_summaries.append(image_summarize(base64_image, prompt))\n",
    "\n",
    "    return img_base64_list, image_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "762f47e8-13bc-49b2-8271-681fd2fdc073",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'timeout('The write operation timed out')': /runs\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'timeout('The write operation timed out')': /runs\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'timeout('The write operation timed out')': /runs\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'timeout('The write operation timed out')': /runs\n"
     ]
    }
   ],
   "source": [
    "# Temp: Store images from papers here\n",
    "base_directory = \"/Users/rlm/Desktop/semi_structured_reports/\"\n",
    "img_base64_list, image_summaries = generate_img_summaries(base_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d6be622-b272-46bd-bd23-590e21c0a281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vectorstore to use to index the summaries\n",
    "multi_modal_vectorstore = Chroma(\n",
    "    collection_name=\"multi_modal\", embedding_function=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "retriever_multi_vector_img = create_multi_vector_retriever(\n",
    "    multi_modal_vectorstore,\n",
    "    image_summaries=image_summaries,\n",
    "    images=img_base64_list,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f61ab576-8a84-4a83-a443-4e024e6dfd8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The image is a financial document, specifically the \"Condensed Consolidated Statements of Operations\" for Datadog, Inc. It shows a table with monetary figures in thousands, comparing performance over three and nine months ending September 30, 2023, and 2022. The table includes revenues, costs, operating expenses broken down into research and development, sales and marketing, and general and administrative expenses. It also details operating loss, other income or loss, provision for income taxes, and net income or loss both basic and diluted. Additional notes explain stock-based compensation expense, amortization of acquired intangibles, employer payroll taxes on employee stock transactions, and amortization of issuance costs. The document is detailed and would be relevant to financial analysts, investors, or anyone interested in the company\\'s financial performance.',\n",
       " \"This is an image of a condensed consolidated balance sheet for Datadog, Inc., showing financial data as of September 30, 2023, and December 31, 2022. The balance sheet includes assets, liabilities, and stockholders' equity, with figures in thousands and unverified. The total assets have increased from December 2022 to September 2023, as have the total liabilities and stockholders' equity.\",\n",
       " 'This image is a financial document titled \"Condensed Consolidated Statements of Cash Flow\" for Datadog, Inc., showing cash flow activities in thousands (unaudited) for the three and nine months ended September 30, 2023, and 2022. The document details cash flows from operating, investing, and financing activities, as well as the net increase/decrease in cash and cash equivalents, and provides a reconciliation of cash within the balance sheets.',\n",
       " 'The image is a financial statement titled \"Reconciliation from GAAP to Non-GAAP Results\" from Datadog, Inc., detailing the company\\'s financial performance for the three and nine months ended September 30, 2023. It includes reconciliations of gross profit, operating expenses, sales and marketing costs, general and administrative expenses, operating income, and net income. Both GAAP (Generally Accepted Accounting Principles) and Non-GAAP (adjusted financial measures) figures are presented, including adjustments for stock-based compensation expense, amortization of acquired intangibles, and employer payroll taxes on employee stock transactions. The document provides gross margin, operating margin, and net income per share data, along with the number of shares used in non-GAAP net income per share calculations.',\n",
       " 'The image displays Table I, titled \"Bank Failures 2001-2023,\" showing the number of bank failures, assets, and deposits for two periods: 2001-2020 and 2021-2023. For 2001-2020, there were 561 bank failures with $721 billion in assets and $522 billion in deposits. For the period 2021-2023, there were 2 bank failures, with assets totaling $319 billion and deposits amounting to $264 billion. The source of the data is CRS with data from FDIC, Bank Failures in Brief, and the notes indicate the information is current as of March 22, 2023.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a94ea8f-eb08-492b-90db-96e27ccd5b61",
   "metadata": {},
   "source": [
    "## RAG\n",
    "\n",
    "### Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcc530b9-c804-4354-967f-4342431eccd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "def rag_chain(retriever):\n",
    "    \"\"\"\n",
    "    RAG chain\n",
    "    \"\"\"\n",
    "\n",
    "    # Prompt template\n",
    "    template = \"\"\"Answer the question based only on the following context, which can include text and tables:\n",
    "    {context}\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    # LLM\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
    "\n",
    "    # RAG pipeline\n",
    "    chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    return chain\n",
    "\n",
    "\n",
    "# Create RAG chains\n",
    "chain_baseline = rag_chain(retriever_baseline)\n",
    "chain_unstructured = rag_chain(retriever_unstructured)\n",
    "# chain_docugami = rag_chain(retriever_docugami)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43352e05-ac07-4130-be85-602c1b0f3f40",
   "metadata": {},
   "source": [
    "### Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b6465d2-f770-4ea3-bd9d-7d8419200917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def plt_img_base64(img_base64):\n",
    "    \"\"\"Disply base64 encoded string as image\"\"\"\n",
    "    # Create an HTML img tag with the base64 string as the source\n",
    "    image_html = f'<img src=\"data:image/jpeg;base64,{img_base64}\" />'\n",
    "    # Display the image by rendering the HTML\n",
    "    display(HTML(image_html))\n",
    "\n",
    "\n",
    "def looks_like_base64(sb):\n",
    "    \"\"\"Check if the string looks like base64\"\"\"\n",
    "    return re.match(\"^[A-Za-z0-9+/]+[=]{0,2}$\", sb) is not None\n",
    "\n",
    "\n",
    "def is_image_data(b64data):\n",
    "    \"\"\"\n",
    "    Check if the base64 data is an image by looking at the start of the data\n",
    "    \"\"\"\n",
    "    image_signatures = {\n",
    "        b\"\\xFF\\xD8\\xFF\": \"jpg\",\n",
    "        b\"\\x89\\x50\\x4E\\x47\\x0D\\x0A\\x1A\\x0A\": \"png\",\n",
    "        b\"\\x47\\x49\\x46\\x38\": \"gif\",\n",
    "        b\"\\x52\\x49\\x46\\x46\": \"webp\",\n",
    "    }\n",
    "    try:\n",
    "        header = base64.b64decode(b64data)[:8]  # Decode and get the first 8 bytes\n",
    "        for sig, format in image_signatures.items():\n",
    "            if header.startswith(sig):\n",
    "                return True\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def resize_base64_image(base64_string, size=(128, 128)):\n",
    "    \"\"\"\n",
    "    Resize an image encoded as a Base64 string\n",
    "    \"\"\"\n",
    "    # Decode the Base64 string\n",
    "    img_data = base64.b64decode(base64_string)\n",
    "    img = Image.open(io.BytesIO(img_data))\n",
    "\n",
    "    # Resize the image\n",
    "    resized_img = img.resize(size, Image.LANCZOS)\n",
    "\n",
    "    # Save the resized image to a bytes buffer\n",
    "    buffered = io.BytesIO()\n",
    "    resized_img.save(buffered, format=img.format)\n",
    "\n",
    "    # Encode the resized image to Base64\n",
    "    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def split_image_text_types(docs):\n",
    "    \"\"\"\n",
    "    Split base64-encoded images and texts\n",
    "    \"\"\"\n",
    "    b64_images = []\n",
    "    texts = []\n",
    "    for doc in docs:\n",
    "        # Check if the document is of type Document and extract page_content if so\n",
    "        if isinstance(doc, Document):\n",
    "            doc = doc.page_content\n",
    "        if looks_like_base64(doc) and is_image_data(doc):\n",
    "            doc = resize_base64_image(doc, size=(1300, 600))\n",
    "            b64_images.append(doc)\n",
    "        else:\n",
    "            texts.append(doc)\n",
    "    return {\"images\": b64_images, \"texts\": texts}\n",
    "\n",
    "\n",
    "def img_prompt_func(data_dict):\n",
    "    \"\"\"\n",
    "    Join the context into a single string\n",
    "    \"\"\"\n",
    "    formatted_texts = \"\\n\".join(data_dict[\"context\"][\"texts\"])\n",
    "    messages = []\n",
    "\n",
    "    # Adding image(s) to the messages if present\n",
    "    if data_dict[\"context\"][\"images\"]:\n",
    "        for image in data_dict[\"context\"][\"images\"]:\n",
    "            image_message = {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n",
    "            }\n",
    "            messages.append(image_message)\n",
    "\n",
    "    # Adding the text for analysis\n",
    "    text_message = {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": (\n",
    "            \"You are an analyst tasking with providing advice basded on images.\\n\"\n",
    "            \"You may be given a mix of text, tables, and image(s) usually of charts or graphs.\\n\"\n",
    "            \"Use this information to provide analysis related to the user question. \\n\"\n",
    "            f\"User-provided question: {data_dict['question']}\\n\\n\"\n",
    "            \"Text and / or tables:\\n\"\n",
    "            f\"{formatted_texts}\"\n",
    "        ),\n",
    "    }\n",
    "    messages.append(text_message)\n",
    "    return [HumanMessage(content=messages)]\n",
    "\n",
    "\n",
    "def multi_modal_rag_chain(retriever):\n",
    "    \"\"\"\n",
    "    Multi-modal RAG chain\n",
    "    \"\"\"\n",
    "\n",
    "    # Multi-modal LLM\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4-vision-preview\", max_tokens=1024)\n",
    "\n",
    "    # RAG pipeline\n",
    "    chain = (\n",
    "        {\n",
    "            \"context\": retriever | RunnableLambda(split_image_text_types),\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "        | RunnableLambda(img_prompt_func)\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return chain\n",
    "\n",
    "\n",
    "chain_multimodal = multi_modal_rag_chain(retriever_multi_vector_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092b9c4a-ed00-4437-acc6-13f8cb832e81",
   "metadata": {},
   "source": [
    "## Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85c1456-3b26-42d8-bc91-36f52082e13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check retrieval\n",
    "query = \"Analyzing the operating expenses for Q3 2023, which category saw the largest increase when compared to Q3 2022?\"\n",
    "docs = retriever_multi_vector_img.get_relevant_documents(query, limit=6)\n",
    "\n",
    "# Check that we get back relevant images\n",
    "plt_img_base64(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ebcbab-3207-4012-82b2-9d71d0f0edb1",
   "metadata": {},
   "source": [
    "# Eval\n",
    "\n",
    "See guide [here](https://github.com/langchain-ai/langchain-benchmarks/blob/main/docs/source/notebooks/retrieval/semi_structured.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cdfdd7bc-a95b-4107-9164-70af9ff8ebdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eb27d319827470789d7c8533cabd6b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished fetching examples. Creating dataset...\n",
      "New dataset created you can access it at https://smith.langchain.com/o/1fa8b1f4-fcb9-4072-9aa9-983e35ad61b8/datasets/01a8ff52-a089-43a8-aef4-281342846932.\n",
      "Done creating dataset.\n"
     ]
    }
   ],
   "source": [
    "clone_public_dataset(task.dataset_id, \n",
    "                     dataset_name=task.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d729313-5f39-4a4d-87d3-8cd837ad7d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'bold-stone-35' at:\n",
      "https://smith.langchain.com/o/1fa8b1f4-fcb9-4072-9aa9-983e35ad61b8/projects/p/4d2ce2ec-4028-40cd-8d4e-ac7622b4c972?eval=true\n",
      "\n",
      "View all tests for Dataset Semi-structured Reports at:\n",
      "https://smith.langchain.com/o/1fa8b1f4-fcb9-4072-9aa9-983e35ad61b8/datasets/01a8ff52-a089-43a8-aef4-281342846932\n",
      "[>                                                 ] 0/5"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain.smith.evaluation.runner_utils:Chain failed for example b9a43a22-015e-4ba7-96a5-8601a1491690 with inputs {'question': 'Analyzing the operating expenses for Q3 2023, which category saw the largest increase when compared to Q3 2022?'}\n",
      "Error Type: TypeError, Message: expected string or buffer\n",
      "WARNING:langchain.smith.evaluation.runner_utils:Chain failed for example ddb1153b-c1ea-4e65-88ee-e17c2f6b0314 with inputs {'question': 'What is the change in cash flow from operations for Q3 2023 versus Q3 2022?'}\n",
      "Error Type: TypeError, Message: expected string or buffer\n",
      "WARNING:langchain.smith.evaluation.runner_utils:Chain failed for example 1cf7562f-b2a3-408a-9db1-1f961586fb40 with inputs {'question': 'How many bank failures were there between 2021 and 2023?'}\n",
      "Error Type: TypeError, Message: expected string or buffer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[----------------------------->                    ] 3/5"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain.smith.evaluation.runner_utils:Chain failed for example 7ff47612-ecbc-4d28-9ab2-8a015d671630 with inputs {'question': 'What is the difference in deposits for bank failures in 2001-2020 versus 2021-2023?'}\n",
      "Error Type: TypeError, Message: expected string or buffer\n",
      "WARNING:langchain.smith.evaluation.runner_utils:Chain failed for example 3a84c56e-0afe-462c-a701-57ed46ae0e1b with inputs {'question': \"Based on the net income (loss) reported for Q3 2023, how did Datadog's performance change compared to Q3 2022, and what might this indicate in terms of their financial recovery?\"}\n",
      "Error Type: TypeError, Message: expected string or buffer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------------------------------------------->] 5/5"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'EvalError' object has no attribute 'error'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/Code/langchain-main/langchain/libs/langchain/langchain/smith/evaluation/runner_utils.py:143\u001b[0m, in \u001b[0;36mEvalError.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'error'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m run_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(uuid\u001b[38;5;241m.\u001b[39muuid4())\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m project_name, chain \u001b[38;5;129;01min\u001b[39;00m chain_map\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 31\u001b[0m     \u001b[43mrun_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproject_name\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mrun_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 13\u001b[0m, in \u001b[0;36mrun_eval\u001b[0;34m(chain, eval_run_name)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03mRun eval\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     12\u001b[0m client \u001b[38;5;241m=\u001b[39m Client()\n\u001b[0;32m---> 13\u001b[0m test_run \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_on_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm_or_chain_factory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_eval_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/llama2/lib/python3.9/site-packages/langsmith/client.py:2548\u001b[0m, in \u001b[0;36mClient.run_on_dataset\u001b[0;34m(self, dataset_name, llm_or_chain_factory, evaluation, concurrency_level, project_name, project_metadata, verbose, tags, input_mapper)\u001b[0m\n\u001b[1;32m   2543\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m   2544\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m   2545\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe client.run_on_dataset function requires the langchain\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2546\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpackage to run.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mInstall with pip install langchain\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2547\u001b[0m     )\n\u001b[0;32m-> 2548\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_run_on_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2549\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2550\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm_or_chain_factory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_or_chain_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2551\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconcurrency_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcurrency_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2552\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2553\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2554\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2556\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2557\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_mapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_mapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2558\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Code/langchain-main/langchain/libs/langchain/langchain/smith/evaluation/runner_utils.py:1176\u001b[0m, in \u001b[0;36mrun_on_dataset\u001b[0;34m(client, dataset_name, llm_or_chain_factory, evaluation, concurrency_level, project_name, project_metadata, verbose, tags, **kwargs)\u001b[0m\n\u001b[1;32m   1163\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m runnable_config\u001b[38;5;241m.\u001b[39mget_executor_for_config(configs[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m   1164\u001b[0m         batch_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m   1165\u001b[0m             executor\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m   1166\u001b[0m                 functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1173\u001b[0m             )\n\u001b[1;32m   1174\u001b[0m         )\n\u001b[0;32m-> 1176\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43m_collect_test_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproject_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m   1178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/Code/langchain-main/langchain/libs/langchain/langchain/smith/evaluation/runner_utils.py:1028\u001b[0m, in \u001b[0;36m_collect_test_results\u001b[0;34m(examples, batch_results, configs, project_name)\u001b[0m\n\u001b[1;32m   1022\u001b[0m results[\u001b[38;5;28mstr\u001b[39m(example\u001b[38;5;241m.\u001b[39mid)] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1023\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: example\u001b[38;5;241m.\u001b[39minputs,\n\u001b[1;32m   1024\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeedback\u001b[39m\u001b[38;5;124m\"\u001b[39m: feedback,\n\u001b[1;32m   1025\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexecution_time\u001b[39m\u001b[38;5;124m\"\u001b[39m: all_execution_time\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mstr\u001b[39m(example\u001b[38;5;241m.\u001b[39mid)),\n\u001b[1;32m   1026\u001b[0m }\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, EvalError):\n\u001b[0;32m-> 1028\u001b[0m     results[\u001b[38;5;28mstr\u001b[39m(example\u001b[38;5;241m.\u001b[39mid)][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1030\u001b[0m     results[\u001b[38;5;28mstr\u001b[39m(example\u001b[38;5;241m.\u001b[39mid)][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m output\n",
      "File \u001b[0;32m~/Desktop/Code/langchain-main/langchain/libs/langchain/langchain/smith/evaluation/runner_utils.py:145\u001b[0m, in \u001b[0;36mEvalError.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvalError\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'EvalError' object has no attribute 'error'"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "from langsmith.client import Client\n",
    "\n",
    "from langchain_benchmarks.rag import get_eval_config\n",
    "\n",
    "\n",
    "def run_eval(chain, eval_run_name):\n",
    "    \"\"\"\n",
    "    Run eval\n",
    "    \"\"\"\n",
    "    client = Client()\n",
    "    test_run = client.run_on_dataset(\n",
    "        dataset_name=task.name,\n",
    "        llm_or_chain_factory=chain,\n",
    "        evaluation=get_eval_config(),\n",
    "        verbose=True,\n",
    "        # project_name = eval_run_name\n",
    "    )\n",
    "\n",
    "\n",
    "# Experiments\n",
    "chain_map = {\n",
    "    \"baseline\": chain_baseline,\n",
    "    \"unstructured\": chain_unstructured,\n",
    "    \"multi_modal\": chain_multimodal,\n",
    "    ### \"docugami\": chain_docugami,\n",
    "}\n",
    "\n",
    "run_id = str(uuid.uuid4())\n",
    "for project_name, chain in chain_map.items():\n",
    "    run_eval(chain, project_name+\"_\"+run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186ccb07-4602-4fff-bdfd-48d091e2e872",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
